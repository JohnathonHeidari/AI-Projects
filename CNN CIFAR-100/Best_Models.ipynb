{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363689e-d7e1-4a0a-aeec-5a27153e9806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Dividing the dataset -------------------------------------------------------------------------------------------------------------------\n",
    "CIFAR100 data was downloaded in https://www.cs.utoronto.ca/~kriz/cifar.html in which I use an already existing function called unpickle(file) to \n",
    "extract the nesccary batchfiles containng data, and label in which I'll specific fine” label to grab the 100 different files.\n",
    "\n",
    "Using this data extract I'll specify the train and test data. Then I'll access only key of b'fine_labels', and b'data' as it will be the data need for\n",
    "this assignment\n",
    "\n",
    "The image data of train_x and text_x will converted to pyTorch tensors and reshaped a standard that is require for PyTorch convolutional networks.\n",
    "'255.0' value normalization for pixel values of rgb. Additonal normalization is applied to scale the pixel value  scaling the range to [-1, 1]\n",
    "\n",
    "The TensorDataset is created for both training and test data combine image and label tensor together. The random into a training and a validation set.\n",
    " 80% of the data is kept for training (subtrain_dataset) and the remaining 20% for validation (validation_dataset). Hence the dividing set process.\n",
    "\n",
    " - Selecting the model '2c.#'s' -------------------------------------------------------------------------------------------------------------------\n",
    "    - LeNet1998 modified\n",
    "        - details in Model_1.ipynb\n",
    "    - AlexNet2012 modifed\n",
    "        - detials in Model_2.ipynb\n",
    "    - Simple 2-layer CNN modfied\n",
    "        - details in Model_3.ipynb\n",
    "\n",
    " - Testing for accuracy-------------------------------------------------------------------------------------------------------------------\n",
    "Testing data is extracted through https://www.cs.utoronto.ca/~kriz/cifar.html and will be extracted test_x and test_y \n",
    "these values will be converted into tensor and will be dataloader to 10000 images unseen data that will be tested upon my three models\n",
    "testing for accuracy will check the label of 100 classes and determine if they were correctly predicted\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27262322-f2b2-46f7-ac0d-2feb40420039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cfc62206-0ae6-481c-895f-dd0767ecf760",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', \n",
    "           'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', \n",
    "           'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', \n",
    "           'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', \n",
    "           'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', \n",
    "           'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', \n",
    "           'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', \n",
    "           'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eb9adeaa-593d-44ba-af71-2e1aa371c6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([b'filenames', b'batch_label', b'fine_labels', b'coarse_labels', b'data'])\n",
      "50000\n",
      "[[255 255 255 ...  10  59  79]\n",
      " [255 253 253 ... 253 253 255]\n",
      " [250 248 247 ... 194 207 228]\n",
      " ...\n",
      " [248 240 236 ... 180 174 205]\n",
      " [156 151 151 ... 114 107 126]\n",
      " [ 31  30  31 ...  72  69  67]]\n",
      "(50000, 3072)\n",
      "(3, 32, 32)\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f04403b6d0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#find path to the folder\n",
    "file_path = \"C:/Users/Johnn/OneDrive/Documents/CNN/Models\"\n",
    "data_dir = 'C:/Users/Johnn/OneDrive/Documents/CNN/Models/cifar-100-python'\n",
    "\n",
    "\n",
    "# Function to load a file from the dataset\n",
    "def unpickle(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        dict  = pickle.load(file, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = unpickle(f'{data_dir}/train')\n",
    "test_data = unpickle(f'{data_dir}/test')\n",
    "\n",
    "print(train_data.keys())\n",
    "print(train_data\n",
    "# fine labels identify the class of the image\n",
    "print(len(train_data[b'fine_labels']))\n",
    "# Will access the data values\n",
    "print(train_data[b'data'])\n",
    "print(train_data[b'data'].shape)\n",
    "\n",
    "# visualize the cifar-100 image by reshaping\n",
    "image = train_data[b'data'][0]\n",
    "lab = train_data[b'fine_labels'][0]\n",
    "# divide 3 channels represents into rgb by 32x32 matrixs \n",
    "image = image.reshape(3, 32, 32)\n",
    "print(image.shape)\n",
    "# transpose it to 32 x 32 x 3 \n",
    "image = image.transpose(1, 2, 0)\n",
    "print(image.shape)\n",
    "plt.title(classes[lab])\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44f9efcf-e040-400d-aad1-8f5ba9133824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 50000 , Validation data: 50000\n",
      "<class 'numpy.ndarray'>, <class 'list'>\n",
      "torch.Size([50000, 3, 32, 32])\n",
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Split the training data into a training and a validation set\n",
    "# Accessing the fine_labels from dic will ensure that the model will predict 100 Classes rather than the super-classes\n",
    "# train_x raw pixel data for the images, train_y class identifiers you that it needs to predict\n",
    "train_x, train_y = train_data[b'data'], train_data[b'fine_labels']\n",
    "test_x, test_y = test_data[b'data'], test_data[b'fine_labels']\n",
    "# ensure that the images and specific classes match same length\n",
    "print(f'Train data: {len(train_x)} , Validation data: {len(train_y)}')\n",
    "print(f'{type(train_x)}, {type(train_y)}')\n",
    "# convert to tensors\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32).view(-1, 3, 32, 32) / 255.0\n",
    "train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32).view(-1, 3, 32, 32) / 255.0\n",
    "test_y = torch.tensor(test_y, dtype=torch.long)\n",
    "print(train_x.shape)\n",
    "\n",
    "# Normalize the tensors using the given mean and std\n",
    "mean = torch.tensor([0.5, 0.5, 0.5]).view(1, 3, 1, 1)\n",
    "std = torch.tensor([0.5, 0.5, 0.5]).view(1, 3, 1, 1)\n",
    "train_x = (train_x - mean) / std\n",
    "test_x = (test_x - mean) / std\n",
    "\n",
    "\n",
    "full_dataset = TensorDataset(train_x, train_y)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(len(full_dataset))\n",
    "print(len(test_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad29a19-9933-4ec9-8296-b7ed454806a3",
   "metadata": {},
   "source": [
    "# re-training by Full Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d3dfb95c-2989-43aa-a220-d0664358ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, full_dataset, batch_size, num_epochs):\n",
    "    # Create data loaders for the training and validation sets\n",
    "    fulltrain_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    tr_acc, tr_loss = [], []\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_train = 0\n",
    "        correct_train = 0\n",
    "        for images, labels in fulltrain_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            # Calculate predictions for train accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        train_accuracy = correct_train / total_train\n",
    "        tr_acc.append(train_accuracy)\n",
    "        train_loss = running_loss / len(fulltrain_loader)\n",
    "        tr_loss.append(train_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch}/{num_epochs}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    print('Training Completed')\n",
    "    return tr_acc, tr_loss\n",
    "\n",
    "\n",
    "\n",
    "def test_model(model,test_loader):\n",
    "    #model = model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        n_class_correct = [0 for i in range(100)]\n",
    "        n_class_samples = [0 for i in range(100)]\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = predicted[i]\n",
    "                if (label == pred):\n",
    "                    n_class_correct[label] += 1\n",
    "                n_class_samples[label] += 1\n",
    "                \n",
    "    test_accuracy = 100 * (correct / total)\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}%')\n",
    "    \n",
    "    for i in range(100):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e32caf4d-b7e7-481d-9160-33a384e83834",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb340479-5b54-455a-ae18-835378fc25e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First mode\n",
    "class CNN_LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_LeNet, self).__init__()\n",
    "\n",
    "        # Input size for image CIFAR100 32x32 3 color channel  \n",
    "        # Layer 1: Conv2d + LeakyReLU + Average Pooling\n",
    "\n",
    "        # Input: 32x32x3\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), \n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        # Output size: 16x16x32\n",
    "\n",
    "        # Layer 2: Conv2d + ReLU + Average Pooling\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5), # Increase from 32 to 64 channels\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        # Output size: 6x6x64 (without padding)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=64 * 6 * 6, out_features=240, bias=True),  \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(240)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features=240, out_features=120, bias=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=100)  \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)  # Flatten for the fully connected layer\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out) \n",
    "        out = self.fc3(out) # criterion handle the softmax by the CrossEntropyLoss clasisfcation \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e09f9690-8bc9-4e2f-adfa-0143be08fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model1 = CNN_LeNet().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# Examine the effects in Adam   \n",
    "optimizer1= optim.Adam(model1.parameters(),lr=1e-4, betas=(0.78, 0.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84fc8675-9453-4de3-8440-1fbc39539888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Training Loss: 1.9846, Training Accuracy: 0.4629\n",
      "Epoch 19/100, Training Loss: 1.9650, Training Accuracy: 0.4675\n",
      "Epoch 29/100, Training Loss: 1.9472, Training Accuracy: 0.4702\n",
      "Epoch 39/100, Training Loss: 1.9368, Training Accuracy: 0.4758\n",
      "Epoch 49/100, Training Loss: 1.9204, Training Accuracy: 0.4780\n",
      "Epoch 59/100, Training Loss: 1.9007, Training Accuracy: 0.4812\n",
      "Epoch 69/100, Training Loss: 1.9069, Training Accuracy: 0.4805\n",
      "Epoch 79/100, Training Loss: 1.8907, Training Accuracy: 0.4850\n",
      "Epoch 89/100, Training Loss: 1.8829, Training Accuracy: 0.4879\n",
      "Epoch 99/100, Training Loss: 1.8631, Training Accuracy: 0.4925\n",
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "tr_acc, tr_loss = train_model(model1, optimizer1, criterion, full_dataset, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea07d9f-4639-4869-a4df-6b9f70d4b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking \n",
    "\"\"\"\n",
    "Since the test accuracy is 50.04%\n",
    "My LetNet 1998 model would land at number 32, between Sign-Symmetry and Stochastic Pooling\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c0e3fa0-a345-4584-ba87-e8ea08e83288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 50.0400%\n",
      "Accuracy of apple: 75.0 %\n",
      "Accuracy of aquarium_fish: 64.0 %\n",
      "Accuracy of baby: 32.0 %\n",
      "Accuracy of bear: 22.0 %\n",
      "Accuracy of beaver: 28.0 %\n",
      "Accuracy of bed: 56.0 %\n",
      "Accuracy of bee: 62.0 %\n",
      "Accuracy of beetle: 48.0 %\n",
      "Accuracy of bicycle: 68.0 %\n",
      "Accuracy of bottle: 60.0 %\n",
      "Accuracy of bowl: 34.0 %\n",
      "Accuracy of boy: 19.0 %\n",
      "Accuracy of bridge: 50.0 %\n",
      "Accuracy of bus: 33.0 %\n",
      "Accuracy of butterfly: 34.0 %\n",
      "Accuracy of camel: 35.0 %\n",
      "Accuracy of can: 46.0 %\n",
      "Accuracy of castle: 73.0 %\n",
      "Accuracy of caterpillar: 39.0 %\n",
      "Accuracy of cattle: 42.0 %\n",
      "Accuracy of chair: 72.0 %\n",
      "Accuracy of chimpanzee: 81.0 %\n",
      "Accuracy of clock: 40.0 %\n",
      "Accuracy of cloud: 70.0 %\n",
      "Accuracy of cockroach: 75.0 %\n",
      "Accuracy of couch: 31.0 %\n",
      "Accuracy of crab: 39.0 %\n",
      "Accuracy of crocodile: 52.0 %\n",
      "Accuracy of cup: 75.0 %\n",
      "Accuracy of dinosaur: 46.0 %\n",
      "Accuracy of dolphin: 63.0 %\n",
      "Accuracy of elephant: 50.0 %\n",
      "Accuracy of flatfish: 43.0 %\n",
      "Accuracy of forest: 52.0 %\n",
      "Accuracy of fox: 61.0 %\n",
      "Accuracy of girl: 28.0 %\n",
      "Accuracy of hamster: 49.0 %\n",
      "Accuracy of house: 43.0 %\n",
      "Accuracy of kangaroo: 41.0 %\n",
      "Accuracy of keyboard: 61.0 %\n",
      "Accuracy of lamp: 34.0 %\n",
      "Accuracy of lawn_mower: 63.0 %\n",
      "Accuracy of leopard: 63.0 %\n",
      "Accuracy of lion: 53.0 %\n",
      "Accuracy of lizard: 22.0 %\n",
      "Accuracy of lobster: 34.0 %\n",
      "Accuracy of man: 28.0 %\n",
      "Accuracy of maple_tree: 50.0 %\n",
      "Accuracy of motorcycle: 83.0 %\n",
      "Accuracy of mountain: 61.0 %\n",
      "Accuracy of mouse: 23.0 %\n",
      "Accuracy of mushroom: 38.0 %\n",
      "Accuracy of oak_tree: 76.0 %\n",
      "Accuracy of orange: 83.0 %\n",
      "Accuracy of orchid: 68.0 %\n",
      "Accuracy of otter: 13.0 %\n",
      "Accuracy of palm_tree: 69.0 %\n",
      "Accuracy of pear: 58.0 %\n",
      "Accuracy of pickup_truck: 60.0 %\n",
      "Accuracy of pine_tree: 33.0 %\n",
      "Accuracy of plain: 81.0 %\n",
      "Accuracy of plate: 64.0 %\n",
      "Accuracy of poppy: 54.0 %\n",
      "Accuracy of porcupine: 49.0 %\n",
      "Accuracy of possum: 22.0 %\n",
      "Accuracy of rabbit: 28.0 %\n",
      "Accuracy of raccoon: 33.0 %\n",
      "Accuracy of ray: 35.0 %\n",
      "Accuracy of road: 87.0 %\n",
      "Accuracy of rocket: 64.0 %\n",
      "Accuracy of rose: 59.0 %\n",
      "Accuracy of sea: 74.0 %\n",
      "Accuracy of seal: 19.0 %\n",
      "Accuracy of shark: 37.0 %\n",
      "Accuracy of shrew: 36.0 %\n",
      "Accuracy of skunk: 71.0 %\n",
      "Accuracy of skyscraper: 72.0 %\n",
      "Accuracy of snail: 28.0 %\n",
      "Accuracy of snake: 26.0 %\n",
      "Accuracy of spider: 56.0 %\n",
      "Accuracy of squirrel: 34.0 %\n",
      "Accuracy of streetcar: 64.0 %\n",
      "Accuracy of sunflower: 81.0 %\n",
      "Accuracy of sweet_pepper: 32.0 %\n",
      "Accuracy of table: 37.0 %\n",
      "Accuracy of tank: 71.0 %\n",
      "Accuracy of telephone: 58.0 %\n",
      "Accuracy of television: 57.0 %\n",
      "Accuracy of tiger: 49.0 %\n",
      "Accuracy of tractor: 66.0 %\n",
      "Accuracy of train: 35.0 %\n",
      "Accuracy of trout: 57.0 %\n",
      "Accuracy of tulip: 26.0 %\n",
      "Accuracy of turtle: 27.0 %\n",
      "Accuracy of wardrobe: 85.0 %\n",
      "Accuracy of whale: 53.0 %\n",
      "Accuracy of willow_tree: 48.0 %\n",
      "Accuracy of wolf: 62.0 %\n",
      "Accuracy of woman: 16.0 %\n",
      "Accuracy of worm: 47.0 %\n"
     ]
    }
   ],
   "source": [
    "test_model(model1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ba2934b1-3311-4bb0-a0cf-b2666988eba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_LeNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (4): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (4): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=240, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): BatchNorm1d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=240, out_features=120, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc3): Linear(in_features=120, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "49675229-9e81-40b8-9b1b-daf706a7935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a alex modified alex \n",
    "class CNN_AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_AlexNet, self).__init__()\n",
    "\n",
    "        # Input: 32x32x3\n",
    "        # Layer 1: Conv2d + LeakyReLU + Max Pooling\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            # ((32 - 3 + 2(1))/ 1) + 1 = 32x32   channels 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2), # .3 -> .2\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # 32/2 = 16\n",
    "        )\n",
    "        # Output size:  16x16x64\n",
    "\n",
    "        # Layer 2: Conv2d + LeakyReLU + Max Pooling\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            # ((16 − 3 + 2(0))/1)+1=   16x16x192\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2), \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # 16/2 = 8\n",
    "        )\n",
    "\n",
    "     # Layer 3: Conv2d + LeakyReLU \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            # ((16 − 5 + 2(0))/1)+1=   16x16x384\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.3), \n",
    "\n",
    "        )\n",
    "        \n",
    "     # Layer 4: Conv2d + LeakyReLU \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            # ((16 − 5 + 2(0))/1)+1=    16x16  256\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "        )\n",
    "        \n",
    "        # Layer 5: Conv2d + LeakyReLU + Max Pooling\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=5, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "          # ((16 − 5 + 2(0))/1)+1=    16x16  256\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            # 8/2 = 4\n",
    "        )\n",
    "        # Output size: 5x5x16\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=256 * 2 * 2, out_features=4096, bias=True),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.Dropout1d(0.2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.Dropout1d(0.2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(in_features=4096, out_features=100) \n",
    " \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.view(out.size(0), -1)  # Flatten for the fully connected layer\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out) \n",
    "        out = self.fc3(out) # criterion handle the softmax by the CrossEntropyLoss clasisfcation \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1b1084f-3f7e-4fb5-8e77-7651604804cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model2 = CNN_AlexNet().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer2 = optim.Adam(model2.parameters(),lr=1e-4,weight_decay=2e-3,betas=(0.85, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "081261d0-052d-480f-abdd-13a33e404313",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ca689c9a-f88a-4972-98df-4d834db7a3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/70, Training Loss: 3.0461, Training Accuracy: 0.2812\n",
      "Epoch 19/70, Training Loss: 2.7489, Training Accuracy: 0.3517\n",
      "Epoch 29/70, Training Loss: 2.4647, Training Accuracy: 0.4214\n",
      "Epoch 39/70, Training Loss: 2.2620, Training Accuracy: 0.4758\n",
      "Epoch 49/70, Training Loss: 2.1351, Training Accuracy: 0.5136\n",
      "Epoch 59/70, Training Loss: 2.0749, Training Accuracy: 0.5307\n",
      "Epoch 69/70, Training Loss: 2.0181, Training Accuracy: 0.5498\n",
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "tr_acc, tr_loss = train_model(model2, optimizer2, criterion, full_dataset, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56021299-d284-4e8f-b343-7c2954ede9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking \n",
    "\"\"\"\n",
    "Using no-extra data for training I'll compare my model\n",
    "Since the test accuracy is 54.4%\n",
    "My AlexNet 2012 model would land at number 32, between Sign-Symmetry and Stochastic Pooling\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e73b85db-a6ea-4969-9816-f315134b3ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 54.4000%\n",
      "Accuracy of apple: 79.0 %\n",
      "Accuracy of aquarium_fish: 66.0 %\n",
      "Accuracy of baby: 41.0 %\n",
      "Accuracy of bear: 35.0 %\n",
      "Accuracy of beaver: 41.0 %\n",
      "Accuracy of bed: 53.0 %\n",
      "Accuracy of bee: 61.0 %\n",
      "Accuracy of beetle: 55.0 %\n",
      "Accuracy of bicycle: 64.0 %\n",
      "Accuracy of bottle: 61.0 %\n",
      "Accuracy of bowl: 36.0 %\n",
      "Accuracy of boy: 22.0 %\n",
      "Accuracy of bridge: 48.0 %\n",
      "Accuracy of bus: 41.0 %\n",
      "Accuracy of butterfly: 51.0 %\n",
      "Accuracy of camel: 43.0 %\n",
      "Accuracy of can: 56.0 %\n",
      "Accuracy of castle: 73.0 %\n",
      "Accuracy of caterpillar: 39.0 %\n",
      "Accuracy of cattle: 45.0 %\n",
      "Accuracy of chair: 76.0 %\n",
      "Accuracy of chimpanzee: 63.0 %\n",
      "Accuracy of clock: 57.0 %\n",
      "Accuracy of cloud: 63.0 %\n",
      "Accuracy of cockroach: 73.0 %\n",
      "Accuracy of couch: 44.0 %\n",
      "Accuracy of crab: 52.0 %\n",
      "Accuracy of crocodile: 37.0 %\n",
      "Accuracy of cup: 71.0 %\n",
      "Accuracy of dinosaur: 55.0 %\n",
      "Accuracy of dolphin: 46.0 %\n",
      "Accuracy of elephant: 54.0 %\n",
      "Accuracy of flatfish: 42.0 %\n",
      "Accuracy of forest: 60.0 %\n",
      "Accuracy of fox: 56.0 %\n",
      "Accuracy of girl: 35.0 %\n",
      "Accuracy of hamster: 61.0 %\n",
      "Accuracy of house: 44.0 %\n",
      "Accuracy of kangaroo: 35.0 %\n",
      "Accuracy of keyboard: 69.0 %\n",
      "Accuracy of lamp: 41.0 %\n",
      "Accuracy of lawn_mower: 73.0 %\n",
      "Accuracy of leopard: 58.0 %\n",
      "Accuracy of lion: 65.0 %\n",
      "Accuracy of lizard: 26.0 %\n",
      "Accuracy of lobster: 38.0 %\n",
      "Accuracy of man: 36.0 %\n",
      "Accuracy of maple_tree: 59.0 %\n",
      "Accuracy of motorcycle: 82.0 %\n",
      "Accuracy of mountain: 74.0 %\n",
      "Accuracy of mouse: 31.0 %\n",
      "Accuracy of mushroom: 53.0 %\n",
      "Accuracy of oak_tree: 66.0 %\n",
      "Accuracy of orange: 81.0 %\n",
      "Accuracy of orchid: 64.0 %\n",
      "Accuracy of otter: 26.0 %\n",
      "Accuracy of palm_tree: 76.0 %\n",
      "Accuracy of pear: 55.0 %\n",
      "Accuracy of pickup_truck: 69.0 %\n",
      "Accuracy of pine_tree: 39.0 %\n",
      "Accuracy of plain: 88.0 %\n",
      "Accuracy of plate: 59.0 %\n",
      "Accuracy of poppy: 59.0 %\n",
      "Accuracy of porcupine: 56.0 %\n",
      "Accuracy of possum: 38.0 %\n",
      "Accuracy of rabbit: 29.0 %\n",
      "Accuracy of raccoon: 55.0 %\n",
      "Accuracy of ray: 57.0 %\n",
      "Accuracy of road: 79.0 %\n",
      "Accuracy of rocket: 73.0 %\n",
      "Accuracy of rose: 63.0 %\n",
      "Accuracy of sea: 72.0 %\n",
      "Accuracy of seal: 21.0 %\n",
      "Accuracy of shark: 37.0 %\n",
      "Accuracy of shrew: 47.0 %\n",
      "Accuracy of skunk: 72.0 %\n",
      "Accuracy of skyscraper: 74.0 %\n",
      "Accuracy of snail: 46.0 %\n",
      "Accuracy of snake: 41.0 %\n",
      "Accuracy of spider: 60.0 %\n",
      "Accuracy of squirrel: 32.0 %\n",
      "Accuracy of streetcar: 62.0 %\n",
      "Accuracy of sunflower: 88.0 %\n",
      "Accuracy of sweet_pepper: 41.0 %\n",
      "Accuracy of table: 44.0 %\n",
      "Accuracy of tank: 69.0 %\n",
      "Accuracy of telephone: 50.0 %\n",
      "Accuracy of television: 55.0 %\n",
      "Accuracy of tiger: 55.0 %\n",
      "Accuracy of tractor: 64.0 %\n",
      "Accuracy of train: 60.0 %\n",
      "Accuracy of trout: 67.0 %\n",
      "Accuracy of tulip: 47.0 %\n",
      "Accuracy of turtle: 40.0 %\n",
      "Accuracy of wardrobe: 81.0 %\n",
      "Accuracy of whale: 56.0 %\n",
      "Accuracy of willow_tree: 57.0 %\n",
      "Accuracy of wolf: 58.0 %\n",
      "Accuracy of woman: 16.0 %\n",
      "Accuracy of worm: 57.0 %\n"
     ]
    }
   ],
   "source": [
    "test_model(model2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "34de005e-0f1b-46ea-9c25-5fea55be2ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_AlexNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout2d(p=0.2, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout2d(p=0.2, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout2d(p=0.2, inplace=False)\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout2d(p=0.2, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout1d(p=0.2, inplace=False)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout1d(p=0.2, inplace=False)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (fc3): Linear(in_features=4096, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b93d1d56-1c8e-499c-af6e-f87fb6a3deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Simple2layer_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple2layer_CNN, self).__init__()\n",
    "\n",
    "        # Input size for image CIFAR100 32x32 3 color channel  \n",
    "\n",
    "        # Input: 32x32x3\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # out_channels 32->64\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2),\n",
    "            # 32/2 = 16 width, height\n",
    "        )\n",
    "        # Input: 16x16x64\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.2),\n",
    "            # 16/2 = 8x8 channels 128 \n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=128 * 8 * 8, out_features=64),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=100)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)  # Flatten for the fully connected layer\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out) # criterion handle the softmax by the CrossEntropyLoss clasisfcation \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ae17e324-b12e-4132-82b3-8ad7b4b9aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model3 = Simple2layer_CNN().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer3 = optim.RMSprop(model3.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3084b0af-b84d-4987-a437-e4e47b7f3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f6f28c7a-052f-4210-b7c3-2937a1547f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Training Loss: 1.9138, Training Accuracy: 0.5784\n",
      "Epoch 19/100, Training Loss: 1.9105, Training Accuracy: 0.5790\n",
      "Epoch 29/100, Training Loss: 1.8973, Training Accuracy: 0.5832\n",
      "Epoch 39/100, Training Loss: 1.9049, Training Accuracy: 0.5822\n",
      "Epoch 49/100, Training Loss: 1.8959, Training Accuracy: 0.5834\n",
      "Epoch 59/100, Training Loss: 1.8962, Training Accuracy: 0.5847\n",
      "Epoch 69/100, Training Loss: 1.8976, Training Accuracy: 0.5849\n",
      "Epoch 79/100, Training Loss: 1.8871, Training Accuracy: 0.5858\n",
      "Epoch 89/100, Training Loss: 1.8961, Training Accuracy: 0.5843\n",
      "Epoch 99/100, Training Loss: 1.8836, Training Accuracy: 0.5876\n",
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "tr_acc, tr_loss = train_model(model2, optimizer2, criterion, full_dataset, batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02807c57-e03d-4980-a07b-f81f01e1b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking \n",
    "\"\"\"\n",
    "Since the test accuracy is 52.32%\n",
    "My Simple 2-layer CNN model would land at number 32, between Sign-Symmetry and Stochastic Pooling\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b7915d8c-f22b-4a37-8299-0b22d4070a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 52.3200%\n",
      "Accuracy of apple: 77.0 %\n",
      "Accuracy of aquarium_fish: 68.0 %\n",
      "Accuracy of baby: 40.0 %\n",
      "Accuracy of bear: 26.0 %\n",
      "Accuracy of beaver: 37.0 %\n",
      "Accuracy of bed: 52.0 %\n",
      "Accuracy of bee: 55.0 %\n",
      "Accuracy of beetle: 52.0 %\n",
      "Accuracy of bicycle: 62.0 %\n",
      "Accuracy of bottle: 73.0 %\n",
      "Accuracy of bowl: 33.0 %\n",
      "Accuracy of boy: 25.0 %\n",
      "Accuracy of bridge: 47.0 %\n",
      "Accuracy of bus: 39.0 %\n",
      "Accuracy of butterfly: 61.0 %\n",
      "Accuracy of camel: 46.0 %\n",
      "Accuracy of can: 46.0 %\n",
      "Accuracy of castle: 62.0 %\n",
      "Accuracy of caterpillar: 48.0 %\n",
      "Accuracy of cattle: 44.0 %\n",
      "Accuracy of chair: 80.0 %\n",
      "Accuracy of chimpanzee: 75.0 %\n",
      "Accuracy of clock: 55.0 %\n",
      "Accuracy of cloud: 75.0 %\n",
      "Accuracy of cockroach: 81.0 %\n",
      "Accuracy of couch: 37.0 %\n",
      "Accuracy of crab: 49.0 %\n",
      "Accuracy of crocodile: 35.0 %\n",
      "Accuracy of cup: 72.0 %\n",
      "Accuracy of dinosaur: 53.0 %\n",
      "Accuracy of dolphin: 52.0 %\n",
      "Accuracy of elephant: 48.0 %\n",
      "Accuracy of flatfish: 45.0 %\n",
      "Accuracy of forest: 39.0 %\n",
      "Accuracy of fox: 48.0 %\n",
      "Accuracy of girl: 37.0 %\n",
      "Accuracy of hamster: 53.0 %\n",
      "Accuracy of house: 42.0 %\n",
      "Accuracy of kangaroo: 38.0 %\n",
      "Accuracy of keyboard: 74.0 %\n",
      "Accuracy of lamp: 37.0 %\n",
      "Accuracy of lawn_mower: 73.0 %\n",
      "Accuracy of leopard: 52.0 %\n",
      "Accuracy of lion: 68.0 %\n",
      "Accuracy of lizard: 31.0 %\n",
      "Accuracy of lobster: 43.0 %\n",
      "Accuracy of man: 27.0 %\n",
      "Accuracy of maple_tree: 54.0 %\n",
      "Accuracy of motorcycle: 72.0 %\n",
      "Accuracy of mountain: 69.0 %\n",
      "Accuracy of mouse: 22.0 %\n",
      "Accuracy of mushroom: 46.0 %\n",
      "Accuracy of oak_tree: 64.0 %\n",
      "Accuracy of orange: 80.0 %\n",
      "Accuracy of orchid: 54.0 %\n",
      "Accuracy of otter: 22.0 %\n",
      "Accuracy of palm_tree: 74.0 %\n",
      "Accuracy of pear: 48.0 %\n",
      "Accuracy of pickup_truck: 69.0 %\n",
      "Accuracy of pine_tree: 46.0 %\n",
      "Accuracy of plain: 87.0 %\n",
      "Accuracy of plate: 54.0 %\n",
      "Accuracy of poppy: 61.0 %\n",
      "Accuracy of porcupine: 52.0 %\n",
      "Accuracy of possum: 35.0 %\n",
      "Accuracy of rabbit: 33.0 %\n",
      "Accuracy of raccoon: 51.0 %\n",
      "Accuracy of ray: 44.0 %\n",
      "Accuracy of road: 73.0 %\n",
      "Accuracy of rocket: 73.0 %\n",
      "Accuracy of rose: 61.0 %\n",
      "Accuracy of sea: 60.0 %\n",
      "Accuracy of seal: 18.0 %\n",
      "Accuracy of shark: 31.0 %\n",
      "Accuracy of shrew: 40.0 %\n",
      "Accuracy of skunk: 75.0 %\n",
      "Accuracy of skyscraper: 77.0 %\n",
      "Accuracy of snail: 54.0 %\n",
      "Accuracy of snake: 43.0 %\n",
      "Accuracy of spider: 52.0 %\n",
      "Accuracy of squirrel: 22.0 %\n",
      "Accuracy of streetcar: 58.0 %\n",
      "Accuracy of sunflower: 78.0 %\n",
      "Accuracy of sweet_pepper: 43.0 %\n",
      "Accuracy of table: 41.0 %\n",
      "Accuracy of tank: 59.0 %\n",
      "Accuracy of telephone: 53.0 %\n",
      "Accuracy of television: 62.0 %\n",
      "Accuracy of tiger: 55.0 %\n",
      "Accuracy of tractor: 58.0 %\n",
      "Accuracy of train: 52.0 %\n",
      "Accuracy of trout: 68.0 %\n",
      "Accuracy of tulip: 35.0 %\n",
      "Accuracy of turtle: 31.0 %\n",
      "Accuracy of wardrobe: 84.0 %\n",
      "Accuracy of whale: 51.0 %\n",
      "Accuracy of willow_tree: 38.0 %\n",
      "Accuracy of wolf: 57.0 %\n",
      "Accuracy of woman: 19.0 %\n",
      "Accuracy of worm: 62.0 %\n"
     ]
    }
   ],
   "source": [
    "test_model(model2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c89d80d9-ef14-47a0-80d2-8130d14e4648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Simple2layer_CNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=8192, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (fc2): Linear(in_features=64, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6830d-0298-4481-954d-6793c5bb8ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
